{
    "collab_server" : "",
    "contents" : "---\ntitle: \"R Notebook\"\noutput: html_notebook\n---\n\n\n```{r, include=TRUE, message=FALSE}\nsource('libraries.R')\nsource('functions.R')\n```\n\n\n```{r, message=FALSE, include=TRUE}\n\ncol_names <- c('age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n               'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n               'hours_per_week', 'native_country', 'income')\n\n# read the training data \ntrain <- read_csv('data/adult_train.csv', na = '?', col_names = col_names, skip = 1)\n\n# read the testing data\ntest <- read_csv('data/adult_test', \n                 na = '?', \n                 col_names = col_names)\n```\n\n# EDA\n\n```{r}\n# take a peak at the data and the data type of each variable\nstr(data.frame(train))\n```\n\n```{r}\n# check the number of NAs in each column\nnum_NAS(train)\n```\n\n```{r}\n# create a new column 'greater_50k that takes a value of 1 if the income is greater than 50k and 0 otherwise \n\ntrain$greater_50k <- ifelse(train$income == \">50K\", 1, 0)\ntest$greater_50k <- ifelse(test$income == \">50K.\", 1, 0)\n```\n\n\n```{r}\n# proportion of observations in each Native Country\n\nprop.table(table(train$native_country, useNA = 'always'))[order(table(train$native_country), decreasing = TRUE)][1:5]\n```\n\n\n```{r}\n# since the native country of 89.5% of observations is United States create a new variable 'is_UnitedStates' that takes a value of 1 if the observation is from the United States and 0 otherwise.\n\ntrain$is_UnitedStates <- ifelse(train$native_country == 'United-States', 1, 0)\ntest$is_UnitedStates <- ifelse(test$native_country == 'United-States', 1, 0)\n\ntrain$is_UnitedStates[is.na(train$is_UnitedStates)] <- 1\ntest$is_UnitedStates[is.na(test$is_UnitedStates)] <- 1\n\nhead(train)\n```\n\n\n```{r}\n# summary statistics of all the variables\n\nsummary(train)\n```\n\n```{r}\n# Replace NA values in workclass, occupation and Native Country by the maximum occurring values.\n\n# workclass\ntrain$workclass[is.na(train$workclass)] <- return_max(train$workclass)\ntest$workclass[is.na(test$workclass)] <- return_max(train$workclass)\n\n# occupation\ntrain$occupation[is.na(train$occupation)] <- return_max(train$occupation)\ntest$occupation[is.na(test$occupation)] <- return_max(train$occupation)\n\n# Native Country\ntrain$native_country[is.na(train$native_country)] <- return_max(train$native_country)\ntest$native_country[is.na(test$native_country)] <- return_max(train$native_country)\n\n```\n\n### age\n\n```{r}\n# Histogram with a density curve\nplot_histogram(train, 'age')\n```\n\n\n```{r}\n# boxplot comparing the distributions across the two levels of income\nplot_boxplot(train, 'income', 'age')\n```\n\n\n```{r}\n# Faceted historgram by income\nplot_faceted_histogram(train, 'age', 'income')\n```\n\n### fnlwgt\n\n```{r}\nplot_histogram(train, 'fnlwgt')\n```\n\n```{r}\nplot_boxplot(train, 'income', 'fnlwgt')\n```\n\n```{r}\nplot_faceted_histogram(train, 'fnlwgt', 'income')\n```\n\n### Capital-gain\n\n```{r}\n# Histogram with a density curve\nplot_histogram(train, 'capital_gain')\n```\n\n```{r}\nplot_boxplot(train, 'income', 'capital_gain')\n```\n\n```{r}\nplot_faceted_histogram(train, 'capital_gain', 'income')\n```\n\n### Capital-loss\n\n```{r}\n# Histogram with a density curve\nplot_histogram(train, 'capital_loss')\n```\n\n```{r}\nplot_boxplot(train, 'income', 'capital_loss')\n```\n\n```{r}\nplot_faceted_histogram(train, 'capital_loss', 'income')\n```\n\n### Hours-per-week\n\n```{r}\n# Histogram with a density curve\nplot_histogram(train, 'hours_per_week')\n```\n\n```{r}\nplot_boxplot(train, 'income', 'hours_per_week')\n```\n\n```{r}\nplot_faceted_histogram(train, 'hours_per_week', 'income')\n```\n\n### Workclass\n\n```{r}\n# Univariate analysis - count of each level in workclass \nplot_bar_plot_count(train, 'workclass')\n```\n\n```{r}\n# proportion of observations that have income greater than 50k for each level\nplot_bar_plot_prop(train, 'workclass', 'greater_50k')\n```\n\n```{r}\n# stacked bar plot - for a given level shows the number of obervations belonging to each income level\nplot_bar_plot_stacked(train, 'workclass', 'income')\n```\n\n\n### Education\n```{r}\nplot_bar_plot_count(train, 'education')\n```\n\n```{r}\nplot_bar_plot_prop(train, 'education', 'greater_50k')\n```\n\n```{r}\nplot_bar_plot_stacked(train, 'education', 'income')\n```\n\n### Marital Status\n```{r}\nplot_bar_plot_count(train, 'marital_status')\n```\n\n```{r}\nplot_bar_plot_prop(train, 'marital_status', 'greater_50k')\n```\n\n```{r}\nplot_bar_plot_stacked(train, 'marital_status', 'income')\n```\n\n### Occupation\n\n```{r}\nplot_bar_plot_count(train, 'occupation')\n```\n\n```{r}\nplot_bar_plot_prop(train, 'occupation', 'greater_50k')\n```\n\n```{r}\nplot_bar_plot_stacked(train, 'occupation', 'income')\n```\n\n### Relationship\n\n```{r}\nplot_bar_plot_count(train, 'relationship')\n```\n\n```{r}\nplot_bar_plot_prop(train, 'relationship', 'greater_50k')\n```\n\n```{r}\nplot_bar_plot_stacked(train, 'relationship', 'income')\n```\n\n### Race\n\n```{r}\nplot_bar_plot_count(train, 'race')\n```\n\n```{r}\nplot_bar_plot_prop(train, 'race', 'greater_50k')\n```\n\n```{r}\nplot_bar_plot_stacked(train, 'race', 'income')\n```\n\n### Sex\n\n```{r}\nplot_bar_plot_count(train, 'sex')\n```\n\n```{r}\nplot_bar_plot_prop(train, 'sex', 'greater_50k')\n```\n\n```{r}\nplot_bar_plot_stacked(train, 'sex', 'income')\n```\n\n```{r}\n# mosaic plot - shows the proportions of observations in each level of the independent variable for each level of the dependent variable\nmosaicplot(income ~ sex, data = train, main = '')\n```\n\n\n### is_UnitedStates\n```{r}\nplot_bar_plot_count(train, 'is_UnitedStates')\n```\n\n```{r}\nplot_bar_plot_prop(train, 'is_UnitedStates', 'greater_50k')\n```\n\n```{r}\nplot_bar_plot_stacked(train, 'is_UnitedStates', 'income')\n```\n\n```{r}\nmosaicplot(income ~ is_UnitedStates, data = train, main = '')\n```\n\n\n### Correllogram\n\n```{r}\n# correlation plot of numeric variables\nnumeric_variables <- c('age', 'fnlwgt', \"capital_gain\", \"capital_loss\", \"hours_per_week\")\n\ncorrplot(cor(train[numeric_variables]), method = \"number\")\n```\n\n# Feature Engineering\n\n```{r}\n# remove the columns that are not required\ntrain <- train %>%\n  select(-education, -native_country, -income)\n\ntest <- test %>%\n  select(-education, -native_country, -income)\n```\n\n```{r}\nsummary(train)\n```\n\n```{r}\ntable(train$marital_status)\n```\n\n\n```{r}\ntrain$workclass <- as.factor(train$workclass)\ntrain$marital_status <- as.factor(train$marital_status)\ntrain$occupation <- as.factor(train$occupation)\ntrain$relationship <- as.factor(train$relationship)\ntrain$race <- as.factor(train$race)\ntrain$sex <- as.factor(train$sex)\n\ntest$workclass <- as.factor(test$workclass)\ntest$marital_status <- as.factor(test$marital_status)\ntest$occupation <- as.factor(test$occupation)\ntest$relationship <- as.factor(test$relationship)\ntest$race <- as.factor(test$race)\ntest$sex <- as.factor(test$sex)\n```\n\n```{r}\n# one hot encode the categorical variables after dropping the first level \n\ndummy <- dummyVars( ~ ., data = train, fullRank = TRUE)\n\ntrain_dummies <- data.frame(predict(dummy, train))\ntest_dummies <- data.frame(predict(dummy, test))\n\nhead(train_dummies)\n```\n\n\n```{r}\n# convert education_num to character since it is not be normalised/scaled\ntrain_dummies$education_num <- as.character(train_dummies$education_num)\ntest_dummies$education_num <- as.character(test_dummies$education_num)\n```\n\n\n```{r}\n# min max scaler to scale variables between 1 and 0\n\n# intialize and learn the parameters of the scaled object\nscaler <- preProcess(train_dummies, method = 'range')\n\n# scale the training set\ntrain_scaled <- predict(scaler, train_dummies)\n\n# scale the test set\ntest_scaled <- predict(scaler, test_dummies)\n```\n\n```{r}\n# all numeric variables have been scaled between 0 and 1\nsummary(train_scaled)\n```\n\n```{r}\n# label encode education_num\ntrain_scaled$education_num <- as.numeric(train_scaled$education_num)\ntest_scaled$education_num <- as.numeric(test_scaled$education_num)\n\nhead(train_scaled)\n```\n\n# Modelling\n\n### Logistic Regression\n\n```{r}\nmodel_glm <- glm(greater_50k ~ ., data = train_scaled, family = binomial(link = 'logit'))\n\n# check the regression coefficients and the significance\nsummary(model_glm)\n```\n\n```{r}\n# predict on the test set\n\nY_pred <- predict(model_glm, newdata = test_scaled, type = 'response')\n\n# select a probability threshold\nthresh <- 0.5\n\n# values greater than thresh are assigned 1 else 0\nY_pred_vals <- ifelse(Y_pred > thresh, 1, 0)\n\n# confusion matrix and some important metrics - precision, recall, sensitivity, specificity and kappa score\ncaret::confusionMatrix(data = Y_pred_vals, reference = test_scaled$greater_50k, positive = '1', mode =  \"everything\")\n```\n\n```{r}\n# AUC\npROC::auc(test_scaled$greater_50k, Y_pred)\n```\n\n\n```{r}\n# plot the AUC curve\npred1 <- ROCR::prediction(Y_pred, test_scaled$greater_50k)\nperf1 <- ROCR::performance(pred1,\"tpr\",\"fpr\")\nplot(perf1)\n```\n\n\n#### Oversampling\n\n```{r}\n# check the distribution of each class\nprop.table(table(train_scaled$greater_50k))\n```\n\n\n```{r}\n# Since there is some amount of imbalance in the number of observations belonging to each class, (75% of the observations are classifed as having income less than 50k), the minority class viz. the income greater than 50k can be oversampled.\n\n# create a temporary dataframe that excludes the dependent variable\n# train_temp_df <- train_scaled %>% select(-greater_50k)\n\nset.seed(1)\nupSampledTrain <- caret::upSample(x = train_scaled %>% select(-greater_50k), \n                                  y = factor(train_scaled$greater_50k), \n                                  yname = 'greater_50k')\n```\n\n```{r}\n# check new distribution of classes in the upsampled set\nprop.table(table(upSampledTrain$greater_50k))\n```\n\n```{r}\n# fit a new logistic regression model on the oversampled data\nmodel_glm_resamp <- glm(greater_50k ~ . -1, \n                        data = upSampledTrain, \n                        family = binomial(link = 'logit'))\n\nY_pred_resamp <- predict(model_glm_resamp, newdata = test_scaled, type = 'response') # test set remains the same\n\nthresh <- 0.5\nY_pred_resamp_vals <- ifelse(Y_pred_resamp > thresh, 1, 0)\n\n# check the score on the original test set. Note that the observations from test set were not oversampled\n\ncaret::confusionMatrix(data = Y_pred_resamp_vals, reference = test_scaled$greater_50k, positive = '1', mode =  \"everything\")\n```\n\n\n### Ridge\n\n```{r}\n\n# Split the data frame into a matrix containing the predictors and a vector containing the response\n\nX_train <- train_scaled %>%\n  select(-greater_50k) %>%\n  as.matrix()\n\ny_train <- as.factor(train_scaled$greater_50k)\n\nX_test <- test_scaled %>%\n  select(-greater_50k) %>%\n  as.matrix()\n\ny_test <- as.factor(test_scaled$greater_50k)\n```\n\n\n```{r}\nlibrary(glmnet)\n\n# define the values for lambda\nlambdas <- 10^seq(3, -2, by = -.1)\n\n# train for different values of lambda\nfit <- glmnet(X_train, y_train, alpha = 0, lambda = lambdas, family = 'binomial')\n```\n\n\n```{r}\n# find optimal value for lambda\ncv_fit <- cv.glmnet(X_train, y_train, alpha = 0, lambda = lambdas, family=\"binomial\")\nopt_lambda <- cv_fit$lambda.min\n\n# extract all the fitted models\nfit <- cv_fit$glmnet.fit\n\n# predict on the test set selecting the optimal value for lambda\ny_pred <- predict(fit, s = opt_lambda, newx = X_test, type = 'response')\n\n# define threshold and find the values based on the predictions\nthresh <- 0.5\nY_pred_vals <- ifelse(y_pred > thresh, 1, 0)\n\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n### Lasso\n\n```{r}\nlibrary(glmnet)\n\n# fit model by taking an arbitary value for alpha\n\nfit <- glmnet(X_train, y_train, family=\"binomial\", alpha=0.05, lambda=0)\n\ny_pred <- predict(fit, newx = X_test, type = 'response')\n\n# define threshold and find the values based on the predictions\nthresh <- 0.5\nY_pred_vals <- ifelse(y_pred > thresh, 1, 0)\n\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n### Elastic Net\n```{r}\nlibrary(glmnet)\n\n# fit model\n\nfit <- glmnet(X_train, y_train, family=\"binomial\", alpha=0.05, lambda=0.001)\n\ny_pred <- predict(fit, newx = X_test, type = 'response')\n\n# define threshold and find the values based on the predictions\nthresh <- 0.5\nY_pred_vals <- ifelse(y_pred > thresh, 1, 0)\n\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n### Linear Discriminant Analysis\n```{r}\nlibrary(MASS)\n\nfit <- lda(greater_50k ~ ., data = train_scaled)\n\n# predict on the test set\nY_pred_vals <- predict(fit, test_scaled)$class\n\n# confusion matrix and some important metrics - precision, recall, sensitivity, specificity and kappa score\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n\n```\n\n### Quadratic Disriminant Analysis\n\n```{r}\n\n# Turns out race_black and race_white are perfectly correlated.\n\n# library(MASS)\n# \n# fit <- qda(greater_50k ~ ., data = train_scaled)\n# \n# # predict on the test set\n# Y_pred_vals <- predict(fit, test_scaled)$class\n# \n# # confusion matrix and some important metrics - precision, recall, sensitivity, specificity and kappa score\n# caret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n\n```\n\n\n### Data Preprocessing for KNN, SVM and Neural Nets\n\n```{r}\n# one hot encode the categorical variables after not dropping the first level \n\ndummy <- dummyVars( ~ ., data = train, fullRank = FALSE)\n\ntrain_dummies <- data.frame(predict(dummy, train))\ntest_dummies <- data.frame(predict(dummy, test))\n\nhead(train_dummies)\n```\n\n\n```{r}\n# convert education_num to character since it is not be normalised/scaled\ntrain_dummies$education_num <- as.character(train_dummies$education_num)\ntest_dummies$education_num <- as.character(test_dummies$education_num)\n```\n\n```{r}\n# min max scaler to scale variables between 1 and 0\n\n# intialize and learn the parameters of the scaled object\nscaler <- preProcess(train_dummies, method = 'range')\n\n# scale the training set\ntrain_scaled <- predict(scaler, train_dummies)\n\n# scale the test set\ntest_scaled <- predict(scaler, test_dummies)\n```\n\n```{r}\n# label encode education_num\ntrain_scaled$education_num <- as.numeric(train_scaled$education_num)\ntest_scaled$education_num <- as.numeric(test_scaled$education_num)\n```\n\n```{r}\nhead(train_scaled)\n```\n\n```{r}\n# Split the data frame into a matrix containing the predictors and a vector containing the response\n\nX_train <- train_scaled %>%\n  dplyr::select(-greater_50k) %>%\n  as.matrix()\n\ny_train <- as.factor(train_scaled$greater_50k)\n\nX_test <- test_scaled %>%\n  dplyr::select(-greater_50k) %>%\n  as.matrix()\n\ny_test <- as.factor(test_scaled$greater_50k)\n```\n\n### K Nearest Neighbours\n\n```{r}\nlibrary(class)\n\n# fit a KNN with the 5 nearest neigbours\n\nY_pred_vals <- knn(X_train, X_test, cl = y_train, k = 5)\n\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n\n```{r}\nlibrary(e1071)\n\nfit <- svm(x = X_train, y = y_train, scale = FALSE, type = 'C-classification')\n\nY_pred_vals <- predict(fit, newdata = X_test)\n\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n### Neural Networks\n\n```{r}\ncheckInstallLoad('nnet')\n\n# Create a new data frame containing the training data\n\ntrain_scaled_nn <- train_scaled\ntrain_scaled_nn$greater_50k <- as.factor(train_scaled_nn$greater_50k)\n\nfit <- nnet(greater_50k ~ ., data = train_scaled_nn, size = 10, rang = 0.5, maxit = 100)\n\nY_pred_vals <- predict(fit, X_test, type = \"class\")\n\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n### Decision Trees\n\n```{r}\n\n# label encode education_num\ntrain_dummies$education_num <- as.numeric(train_dummies$education_num)\ntest_dummies$education_num <- as.numeric(test_dummies$education_num)\n\ncheckInstallLoad(\"rpart\")\n\n# use the unscaled data to train the decision tree classifier\nfit <- rpart(greater_50k ~ ., data = train_dummies, method = \"class\")\n\nY_pred_vals <- predict(fit, newdata = test_dummies, type = \"class\")\n\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n\n```\n\n```{r}\n# plot the decision tree\n\ncheckInstallLoad(\"RColorBrewer\")\ncheckInstallLoad(\"rattle\")\n\nfancyRpartPlot(fit)\n\n```\n\n#### Grid Search\n\n```{r}\n\n# create another data frame that converts the response to factor\n\ntrain_dummies_grid <- train_dummies\ntrain_dummies_grid$greater_50k <- as.factor(train_dummies_grid$greater_50k)\n\n# grid search with 5 fold cross validation\nfitControl <- trainControl(method = \"repeatedcv\",\n                           number = 5,\n                           ## repeated 1 time\n                           repeats = 1)\n\n\n# define the hyperparameters for the grid search\nparam_grid <- data.frame(cp = c(1, 0.1, 0.01, 0.001, 0.001))\n\n# create a grid search object with five folds using accuracy as a metric\nset.seed(1)\ngrid_fit <- train(greater_50k ~ ., data = train_dummies_grid, \n                 method = \"rpart\", \n                 trControl = fitControl,\n                 tuneGrid = param_grid)\n\n# predict using the best model\nY_pred_vals <- predict(grid_fit, newdata = test_dummies)\n\n# performance of the model on the test set\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n### Random Forest\n```{r}\ncheckInstallLoad(\"randomForest\")\n\ntrain_dummies$greater_50k <- as.factor(train_dummies$greater_50k)\n\nfit <- randomForest(greater_50k ~ ., data = train_dummies, ntree = 10)\n\nY_pred_vals <- predict(fit, newdata = test_dummies)\n\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n### Gradient Boosting\n\n```{r}\ncheckInstallLoad(\"gbm\")\n\n# convert the response variable to character\n\ntrain_dummies_gbm <- train_dummies\ntrain_dummies_gbm$greater_50k <- as.character(train_dummies_gbm$greater_50k)\n\nfit <- gbm(greater_50k ~ ., data = train_dummies_gbm)\n\nY_pred <- predict(fit, newdata = test_dummies, n.trees = 500, type = \"response\")\n\n# select a probability threshold\nthresh <- 0.5\n\n# values greater than thresh are assigned 1 else 0\nY_pred_vals <- ifelse(Y_pred > thresh, 1, 0)\n\n# confusion matrix and some important metrics - precision, recall, sensitivity, specificity and kappa score\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n\n```\n\n### Adaboost\n```{r}\nlibrary(adabag)\n\ntrain_dummies_ada <- train_dummies\ntrain_dummies_ada$greater_50k <- as.factor(train_dummies_ada$greater_50k)\n\nfit <- boosting(greater_50k ~ ., data = train_dummies_ada, mfinal = 10)\n\nY_pred_list <- predict(fit, newdata = test_dummies, type = \"class\")\n\n# select a probability threshold\nthresh <- 0.5\n\n# values greater than thresh are assigned 1 else 0\nY_pred_vals <- ifelse(Y_pred_list$prob[,2] > thresh, 1, 0)\n\n# confusion matrix and some important metrics - precision, recall, sensitivity, specificity and kappa score\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n### XGBoost\n\n```{r}\nlibrary(xgboost)\n\n# create a new vector of class labels that contains values in form of 0 and 1\n\ny_train_xgb <- as.numeric(as.character(y_train))\n\nfit <- xgboost(X_train, y_train_xgb, nrounds = 10, verbose = 0, objective = \"binary:logistic\")\n\nY_pred <- predict(fit, newdata = X_test, t)\n\n# select a probability threshold\nthresh <- 0.5\n\n# values greater than thresh are assigned 1 else 0\nY_pred_vals <- ifelse(Y_pred > thresh, 1, 0)\n\n# confusion matrix and some important metrics - precision, recall, sensitivity, specificity and kappa score\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n\n```\n\n### Bagging\n\n```{r}\ncheckInstallLoad(\"randomForest\")\n\ntrain_dummies$greater_50k <- as.factor(train_dummies$greater_50k)\n\nfit <- randomForest(greater_50k ~ ., \n                    data = train_dummies, \n                    ntree = 10, \n                    mtry = sum(names(train_dummies) != \"greater_50k\")) # Include all variables \n\nY_pred_vals <- predict(fit, newdata = test_dummies)\n\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n```\n\n\n### Naive Bayes\n\n```{r}\ncheckInstallLoad('e1071')\n\n# fit a naive bayes model\nfit <- naiveBayes(greater_50k ~ ., data = train_dummies)\n\n# predict on the test set\nY_pred_vals <- predict(fit, test_dummies)\n\n# confusion matrix and some important metrics - precision, recall, sensitivity, specificity and kappa score\ncaret::confusionMatrix(data = Y_pred_vals, reference = y_test, positive = '1', mode =  \"everything\")\n\n```\n",
    "created" : 1506096286289.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1949081692",
    "id" : "D07FDBFB",
    "lastKnownWriteTime" : 1507031572,
    "last_content_update" : 1507031572218,
    "path" : "~/Lemoxo/Machine Learning Notebooks/ML Notebooks - R Files/Classification.Rmd",
    "project_path" : "Classification.Rmd",
    "properties" : {
        "chunk_output_type" : "inline",
        "tempName" : "Untitled2"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}